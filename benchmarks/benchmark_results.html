<!DOCTYPE html>
<html>
<head>
    <title>SignalLLM Benchmark Results</title>
    <style>
        body { font-family: Arial, sans-serif; margin: 20px; }
        h1 { color: #2c3e50; }
        h2 { color: #3498db; }
        .result-section { margin-bottom: 30px; }
        .plot-container { margin: 20px 0; text-align: center; }
        table { border-collapse: collapse; width: 100%; margin: 20px 0; }
        th, td { padding: 12px; text-align: left; border-bottom: 1px solid #ddd; }
        th { background-color: #f2f2f2; }
        tr:hover { background-color: #f5f5f5; }
        .note { background-color: #f8f9fa; padding: 15px; border-left: 4px solid #3498db; margin: 20px 0; }
    </style>
</head>
<body>
    <h1>SignalLLM Benchmark Results</h1>
    
    <div class="note">
        <h3>Implementation Status</h3>
        <p>All four planned benchmarks have been successfully implemented and visualized. The wavelet transform
        implementation has been significantly improved with Metal Performance Shaders (MPS) optimizations for Apple Silicon,
        achieving up to 417× speedup. The coefficient shape mismatch issues have been resolved, ensuring proper tensor
        dimensions throughout the transform process, with no need to fall back to FFT-based implementations.</p>
    </div>
    
    <div class="result-section">
        <h2>1. Memory Usage</h2>
        <p>This section compares the memory efficiency of SignalLLM with standard Transformers.</p>
        <div class="plot-container">
            <img src="memory_usage.png" alt="Memory Usage" style="max-width: 100%;">
        </div>
    </div>
    
    <div class="result-section">
        <h2>2. Parameter Efficiency</h2>
        <p>This section demonstrates the parameter reduction achieved by spectral embeddings compared to standard embeddings.</p>
        <div class="plot-container">
            <img src="parameter_efficiency.png" alt="Parameter Efficiency" style="max-width: 100%;">
        </div>
        
        <div class="plot-container">
            <img src="embedding_efficiency.png" alt="Embedding Parameter Efficiency" style="max-width: 100%;">
            <p><em>Figure: Embedding Parameter Efficiency showing significantly reduced parameter count in spectral embedding (red) compared to standard embedding (blue) across increasing vocabulary sizes.</em></p>
        </div>
        
        <div class="plot-container">
            <img src="parameter_ratio.png" alt="Parameter Reduction Ratio" style="max-width: 100%;">
            <p><em>Figure: Consistent 6× parameter reduction ratio achieved by spectral embedding across all vocabulary sizes.</em></p>
        </div>
        
        <p>As shown in the charts, SignalLLM achieves a 6× reduction in parameters for embedding layers across all vocabulary sizes tested.</p>
    </div>
    
    <div class="result-section">
        <h2>3. Computational Complexity</h2>
        <p>This section compares the computational complexity of SignalLLM (O(n log n)) with standard Transformers (O(n²)).</p>
        <div class="plot-container">
            <img src="complexity_comparison.png" alt="Computational Complexity Comparison" style="max-width: 100%;">
            <p><em>Figure: Computational complexity comparison showing SignalLLM's O(n log n) scaling advantage over traditional transformer's O(n²) complexity.</em></p>
        </div>
        <div class="plot-container">
            <img src="implementation_comparison.png" alt="Implementation Comparison" style="max-width: 100%;">
            <p><em>Figure: Execution time comparison between PyWavelets (blue) and FFT-based (orange) implementations showing the superior performance of the FFT-based approach.</em></p>
        </div>
    </div>
    
    <div class="result-section">
        <h2>3.1 MPS Optimizations for Apple Silicon</h2>
        <p>This section demonstrates the dramatic performance improvements achieved with Metal Performance Shaders (MPS) optimizations on Apple Silicon hardware.</p>
        
        <div class="note">
            <h3>Key Findings</h3>
            <p>Our MPS-optimized wavelet transform implementation achieves approximately 400× speedup compared to the standard implementation, with consistent performance across various sequence lengths.</p>
        </div>
        
        <table>
            <tr>
                <th>Sequence Length</th>
                <th>Standard (ms)</th>
                <th>MPS Optimized (ms)</th>
                <th>Speedup</th>
            </tr>
            <tr>
                <td>64</td>
                <td>490.70</td>
                <td>1.35</td>
                <td>364.46×</td>
            </tr>
            <tr>
                <td>128</td>
                <td>489.28</td>
                <td>1.19</td>
                <td>410.58×</td>
            </tr>
            <tr>
                <td>256</td>
                <td>495.28</td>
                <td>1.19</td>
                <td>417.04×</td>
            </tr>
            <tr>
                <td>512</td>
                <td>496.58</td>
                <td>1.22</td>
                <td>406.13×</td>
            </tr>
            <tr>
                <td>1024</td>
                <td>493.03</td>
                <td>1.25</td>
                <td>395.97×</td>
            </tr>
        </table>
        
        <div class="plot-container">
            <img src="mps_implementation_comparison.png" alt="MPS Implementation Comparison" style="max-width: 100%;">
            <p><em>Figure: Log-scale comparison between standard wavelet transform implementation (blue) and MPS-optimized implementation (orange), showing dramatic performance improvement.</em></p>
        </div>
        
        <div class="plot-container">
            <img src="mps_wavelet_speedup.png" alt="MPS Speedup Factors" style="max-width: 100%;">
            <p><em>Figure: Speedup factors achieved by MPS optimizations across different sequence lengths, consistently around 400×.</em></p>
        </div>
        
        <div class="plot-container">
            <img src="wavelet_reconstruction_errors.png" alt="Reconstruction Error Analysis" style="max-width: 100%;">
            <p><em>Figure: Reconstruction error analysis showing mean squared error (MSE) between original and reconstructed signals across different sequence lengths.</em></p>
        </div>
        
        <div class="plot-container">
            <img src="wavelet_visualization_256.png" alt="Wavelet Visualization" style="max-width: 100%;">
            <p><em>Figure: Visualization of wavelet decomposition and reconstruction with the optimized implementation, showing original signal (top), decomposition components (middle), and reconstructed signal (bottom).</em></p>
        </div>
        
        <div class="plot-container">
            <img src="wavelet_2d_visualization.png" alt="2D Wavelet Visualization" style="max-width: 100%;">
            <p><em>Figure: 2D wavelet transform visualization showing original pattern (left), approximation coefficients (middle), and reconstructed pattern (right).</em></p>
        </div>
        
        <h3>Key Optimizations</h3>
        <ul>
            <li><strong>Custom Convolution Operations:</strong> Implemented specialized MPS-optimized convolution operations that leverage Metal's computational efficiency</li>
            <li><strong>Coefficient Shape Management:</strong> Fixed shape mismatches by calculating and enforcing proper coefficient sizes at each transform level</li>
            <li><strong>Sequence Length Preservation:</strong> Ensured input and output tensor shapes match exactly for reliable model integration</li>
            <li><strong>Device Handling:</strong> Improved tensor device management to prevent CPU/GPU transfers during computation</li>
        </ul>
    </div>
    
    <div class="result-section">
        <h2>4. NLP Task Performance</h2>
        <p>This section compares the language modeling performance of both architectures.</p>
        <div class="plot-container">
            <img src="nlp_performance.png" alt="NLP Task Performance" style="max-width: 100%;">
            <p><em>Figure: Training loss and perplexity comparison between standard transformer and SignalLLM.</em></p>
        </div>
        <div class="plot-container">
            <img src="training_time.png" alt="Training Time Comparison" style="max-width: 100%;">
            <p><em>Figure: Total training time comparison showing computational efficiency.</em></p>
        </div>
        <div class="plot-container">
            <img src="convergence_rate.png" alt="Convergence Rate" style="max-width: 100%;">
            <p><em>Figure: Convergence rate comparison showing how quickly each model approaches its best performance.</em></p>
        </div>
        <div class="plot-container">
            <img src="training_speedup.png" alt="Training Speedup" style="max-width: 100%;">
            <p><em>Figure: Training speedup ratio of SignalLLM compared to standard transformer.</em></p>
        </div>
    </div>
        
</body>
</html>
    